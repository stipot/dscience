{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение без учителя\n",
    "\n",
    "## Кластеризация\n",
    "\n",
    "В задачах обучения без учителя у нас есть только входные данные x(i) без меток, и мы хотим, чтобы алгоритм нашел некоторую структуру в данных.\n",
    "\n",
    "Алгоритм кластеризации, такой как алгоритм k-средних, пытается сгруппировать данные в k «кластеров», которые имеют некоторое сходство.\n",
    "\n",
    "Примеры: анализ социальных сетей, организация компьютерных кластеров и анализ астрономических данных.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Means\n",
    "Алгоритм кластеризации K-Means использует итеративное уточнение для получения окончательного результата. Входными данными алгоритма являются количество кластеров К и набор данных. Набор данных представляет собой набор функций для каждой точки данных. Алгоритмы начинаются с начальных оценок для К-центроидов, которые могут быть либо сгенерированы случайным образом, либо случайным образом выбраны из набора данных. Затем алгоритм повторяется между двумя шагами:\n",
    "\n",
    "1. Шаг присвоения данных:\n",
    "\n",
    "Каждый центроид определяет один из кластеров. На этом этапе каждой точке данных присваивается ее ближайший центр тяжести на основе квадрата евклидова расстояния. Более формально, если ci представляет собой набор центроидов в наборе C, то каждая точка данных x назначается кластеру на основе\n",
    "\n",
    "$$\\underset{c_i \\in C}{\\arg\\min} \\; dist(c_i,x)^2$$\n",
    "\n",
    "2. Шаг обновления Centroid:\n",
    "\n",
    "На этом этапе центроиды пересчитываются. Это делается путем получения среднего значения всех точек данных, назначенных кластеру этого центроида.\n",
    "\n",
    "$$c_i=\\frac{1}{|S_i|}\\sum_{x_i \\in S_i x_i}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "n_samples = 1500\n",
    "random_state = 1760\n",
    "X, y = make_blobs(n_samples=n_samples, random_state=random_state)\n",
    "\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit_predict(X)\n",
    "y_pred = kmeans.predict(X)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred)\n",
    "print(kmeans.score(X))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как выбрать количество кластеров\n",
    "\n",
    "Метод Elbow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for i in range(1,10):\n",
    "    kmeans = KMeans(n_clusters=i, n_init=\"auto\")\n",
    "    kmeans.fit_predict(X)\n",
    "    scores.append([i,kmeans.score(X)])\n",
    "display(scores)\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(pd.DataFrame(np.array(scores))[0],pd.DataFrame(np.array(scores))[1])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Иерархическая (агломеративная) кластеризация\n",
    "\n",
    "Данные: Информация об атрибутах:\n",
    "\n",
    "1. длина чашелистика в см\n",
    "2. ширина чашелистика в см\n",
    "3. длина лепестка в см\n",
    "4. ширина лепестка в см\n",
    "5. класс:\n",
    "   -- Ирис Сетоса\n",
    "   -- Ирис разноцветный\n",
    "   -- Ирис Вирджиния\n",
    "\n",
    "- Изначально каждая точка сама является кластером.\n",
    "- Два ближайших кластера неоднократно объединяются в один.\n",
    "- Остановка, когда останется только один кластер.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "plt.title(\"Датасет iris\")\n",
    "plt.xlabel(\"Длина Чашели́стика\")\n",
    "plt.ylabel(\"Ширина Чашели́стика\")\n",
    "plt.scatter(X[:, 0], X[:, 1], s=10, c=y, cmap=\"rainbow\")\n",
    "\n",
    "Z = linkage(X, \"ward\")\n",
    "\n",
    "plt.figure(figsize=(25, 10))\n",
    "plt.title(\"Дендрограмма иерархической кластеризации\")\n",
    "plt.xlabel(\"простой индекс\")\n",
    "plt.ylabel(\"дистанция\")\n",
    "\n",
    "dendrogram(Z, leaf_rotation=90.0, leaf_font_size=8.0)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(25, 10))\n",
    "plt.title(\"Дендрограмма иерархической кластеризации\")\n",
    "plt.xlabel(\"простой индекс\")\n",
    "plt.ylabel(\"дистанция\")\n",
    "dendrogram(Z, leaf_rotation=90.0, leaf_font_size=8.0, truncate_mode=\"lastp\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример неприменимости Иерархической кластеризации\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons, make_circles\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "f, ax = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "X, y = make_moons(1000, noise=0.05)\n",
    "cl = AgglomerativeClustering(n_clusters=2).fit(X)\n",
    "ax[0].scatter(X[:, 0], X[:, 1], c=cl.labels_, s=10, cmap=\"rainbow\")\n",
    "\n",
    "X, y = make_circles(1000, factor=0.5, noise=0.05)\n",
    "cl = AgglomerativeClustering(n_clusters=2).fit(X)\n",
    "ax[1].scatter(X[:, 0], X[:, 1], c=cl.labels_, s=10, cmap=\"rainbow\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Спектральна кластеризация\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons, make_circles\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "f, ax = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "X, y = make_moons(1000, noise=0.05)\n",
    "cl = SpectralClustering(n_clusters=2, affinity=\"nearest_neighbors\").fit(X)\n",
    "ax[0].scatter(X[:, 0], X[:, 1], c=cl.labels_, s=10, cmap=\"rainbow\")\n",
    "\n",
    "X, y = make_circles(1000, factor=0.5, noise=0.05)\n",
    "cl = SpectralClustering(n_clusters=2, affinity=\"nearest_neighbors\").fit(X)\n",
    "ax[1].scatter(X[:, 0], X[:, 1], c=cl.labels_, s=10, cmap=\"rainbow\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons, make_circles\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "f, ax = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "X, y = make_moons(1000, noise=0.05)\n",
    "cl = DBSCAN(eps=0.1).fit(X)\n",
    "ax[0].scatter(X[:, 0], X[:, 1], c=cl.labels_, s=10, cmap=\"rainbow\")\n",
    "\n",
    "X, y = make_circles(1000, factor=0.5, noise=0.05)\n",
    "cl = DBSCAN(eps=0.1).fit(X)\n",
    "ax[1].scatter(X[:, 0], X[:, 1], c=cl.labels_, s=10, cmap=\"rainbow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравнительный анализ методов кластеризации\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import cycle, islice\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# ============\n",
    "# Создание наборов данных. Мы выбираем размер достаточно большим, чтобы увидеть масштабируемость\n",
    "# алгоритмов, но не слишком большое, чтобы избежать слишком долгого времени выполнения\n",
    "# ============\n",
    "n_samples = 500\n",
    "noisy_circles = datasets.make_circles(n_samples=n_samples, factor=0.5, noise=0.05)\n",
    "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=0.05)\n",
    "blobs = datasets.make_blobs(n_samples=n_samples, random_state=8)\n",
    "no_structure = np.random.rand(n_samples, 2), None\n",
    "\n",
    "# Анизотропно распределенные данные\n",
    "random_state = 170\n",
    "X, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\n",
    "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
    "X_aniso = np.dot(X, transformation)\n",
    "aniso = (X_aniso, y)\n",
    "\n",
    "# капли с разной дисперсией\n",
    "varied = datasets.make_blobs(\n",
    "    n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state\n",
    ")\n",
    "\n",
    "# ============\n",
    "# Настройте параметры кластера\n",
    "# ============\n",
    "plt.figure(figsize=(9 * 2 + 3, 13))\n",
    "plt.subplots_adjust(\n",
    "    left=0.02, right=0.98, bottom=0.001, top=0.95, wspace=0.05, hspace=0.01\n",
    ")\n",
    "\n",
    "plot_num = 1\n",
    "\n",
    "default_base = {\n",
    "    \"quantile\": 0.3,\n",
    "    \"eps\": 0.3,\n",
    "    \"damping\": 0.9,\n",
    "    \"preference\": -200,\n",
    "    \"n_neighbors\": 3,\n",
    "    \"n_clusters\": 3,\n",
    "    \"min_samples\": 7,\n",
    "    \"xi\": 0.05,\n",
    "    \"min_cluster_size\": 0.1,\n",
    "}\n",
    "\n",
    "datasets = [\n",
    "    (\n",
    "        noisy_circles,\n",
    "        {\n",
    "            \"damping\": 0.77,\n",
    "            \"preference\": -240,\n",
    "            \"quantile\": 0.2,\n",
    "            \"n_clusters\": 2,\n",
    "            \"min_samples\": 7,\n",
    "            \"xi\": 0.08,\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        noisy_moons,\n",
    "        {\n",
    "            \"damping\": 0.75,\n",
    "            \"preference\": -220,\n",
    "            \"n_clusters\": 2,\n",
    "            \"min_samples\": 7,\n",
    "            \"xi\": 0.1,\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        varied,\n",
    "        {\n",
    "            \"eps\": 0.18,\n",
    "            \"n_neighbors\": 2,\n",
    "            \"min_samples\": 7,\n",
    "            \"xi\": 0.01,\n",
    "            \"min_cluster_size\": 0.2,\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        aniso,\n",
    "        {\n",
    "            \"eps\": 0.15,\n",
    "            \"n_neighbors\": 2,\n",
    "            \"min_samples\": 7,\n",
    "            \"xi\": 0.1,\n",
    "            \"min_cluster_size\": 0.2,\n",
    "        },\n",
    "    ),\n",
    "    (blobs, {\"min_samples\": 7, \"xi\": 0.1, \"min_cluster_size\": 0.2}),\n",
    "    (no_structure, {}),\n",
    "]\n",
    "\n",
    "for i_dataset, (dataset, algo_params) in enumerate(datasets):\n",
    "    # обновить параметры со значениями, специфичными для набора данных\n",
    "    params = default_base.copy()\n",
    "    params.update(algo_params)\n",
    "\n",
    "    X, y = dataset\n",
    "\n",
    "    # нормализовать набор данных для облегчения выбора параметров\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # estimate bandwidth for mean shift\n",
    "    bandwidth = cluster.estimate_bandwidth(X, quantile=params[\"quantile\"])\n",
    "\n",
    "    # матрица связности для структурированного Ward\n",
    "    connectivity = kneighbors_graph(\n",
    "        X, n_neighbors=params[\"n_neighbors\"], include_self=False\n",
    "    )\n",
    "    # обеспечить симметричную связность\n",
    "    connectivity = 0.5 * (connectivity + connectivity.T)\n",
    "\n",
    "    # ============\n",
    "    # Создание кластерных объектов\n",
    "    # ============\n",
    "    ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "    two_means = cluster.MiniBatchKMeans(n_clusters=params[\"n_clusters\"], n_init=\"auto\")\n",
    "    ward = cluster.AgglomerativeClustering(\n",
    "        n_clusters=params[\"n_clusters\"], linkage=\"ward\", connectivity=connectivity\n",
    "    )\n",
    "    spectral = cluster.SpectralClustering(\n",
    "        n_clusters=params[\"n_clusters\"],\n",
    "        eigen_solver=\"arpack\",\n",
    "        affinity=\"nearest_neighbors\",\n",
    "    )\n",
    "    dbscan = cluster.DBSCAN(eps=params[\"eps\"])\n",
    "    optics = cluster.OPTICS(\n",
    "        min_samples=params[\"min_samples\"],\n",
    "        xi=params[\"xi\"],\n",
    "        min_cluster_size=params[\"min_cluster_size\"],\n",
    "    )\n",
    "    affinity_propagation = cluster.AffinityPropagation(\n",
    "        damping=params[\"damping\"], preference=params[\"preference\"], random_state=0\n",
    "    )\n",
    "    average_linkage = cluster.AgglomerativeClustering(\n",
    "        linkage=\"average\",\n",
    "        metric=\"cityblock\",\n",
    "        n_clusters=params[\"n_clusters\"],\n",
    "        connectivity=connectivity,\n",
    "    )\n",
    "    birch = cluster.Birch(n_clusters=params[\"n_clusters\"])\n",
    "    gmm = mixture.GaussianMixture(\n",
    "        n_components=params[\"n_clusters\"], covariance_type=\"full\"\n",
    "    )\n",
    "\n",
    "    clustering_algorithms = (\n",
    "        (\"MiniBatch\\nKMeans\", two_means),\n",
    "        (\"Affinity\\nPropagation\", affinity_propagation),\n",
    "        (\"MeanShift\", ms),\n",
    "        (\"Spectral\\nClustering\", spectral),\n",
    "        (\"Ward\", ward),\n",
    "        (\"Agglomerative\\nClustering\", average_linkage),\n",
    "        (\"DBSCAN\", dbscan),\n",
    "        (\"OPTICS\", optics),\n",
    "        (\"BIRCH\", birch),\n",
    "        (\"Gaussian\\nMixture\", gmm),\n",
    "    )\n",
    "\n",
    "    for name, algorithm in clustering_algorithms:\n",
    "        t0 = time.time()\n",
    "\n",
    "        # перехватить предупреждения, связанные с kneighbors_graph\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"the number of connected components of the \"\n",
    "                + \"connectivity matrix is [0-9]{1,2}\"\n",
    "                + \" > 1. Completing it to avoid stopping the tree early.\",\n",
    "                category=UserWarning,\n",
    "            )\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"Graph is not fully connected, spectral embedding\"\n",
    "                + \" may not work as expected.\",\n",
    "                category=UserWarning,\n",
    "            )\n",
    "            algorithm.fit(X)\n",
    "\n",
    "        t1 = time.time()\n",
    "        if hasattr(algorithm, \"labels_\"):\n",
    "            y_pred = algorithm.labels_.astype(int)\n",
    "        else:\n",
    "            y_pred = algorithm.predict(X)\n",
    "\n",
    "        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)\n",
    "        if i_dataset == 0:\n",
    "            plt.title(name, size=18)\n",
    "\n",
    "        colors = np.array(\n",
    "            list(\n",
    "                islice(\n",
    "                    cycle(\n",
    "                        [\n",
    "                            \"#377eb8\",\n",
    "                            \"#ff7f00\",\n",
    "                            \"#4daf4a\",\n",
    "                            \"#f781bf\",\n",
    "                            \"#a65628\",\n",
    "                            \"#984ea3\",\n",
    "                            \"#999999\",\n",
    "                            \"#e41a1c\",\n",
    "                            \"#dede00\",\n",
    "                        ]\n",
    "                    ),\n",
    "                    int(max(y_pred) + 1),\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        # добавить черный цвет для выбросов (если есть)\n",
    "        colors = np.append(colors, [\"#000000\"])\n",
    "        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n",
    "\n",
    "        plt.xlim(-2.5, 2.5)\n",
    "        plt.ylim(-2.5, 2.5)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        plt.text(\n",
    "            0.99,\n",
    "            0.01,\n",
    "            (\"%.2fs\" % (t1 - t0)).lstrip(\"0\"),\n",
    "            transform=plt.gca().transAxes,\n",
    "            size=15,\n",
    "            horizontalalignment=\"right\",\n",
    "        )\n",
    "        plot_num += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тематическое моделирование\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предобработка текста\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "\n",
    "# Загрузка инструментов NLTK\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Инициализация лемматизатора и стоп-слов\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Преобразование в нижний регистр\n",
    "    text = text.lower()\n",
    "\n",
    "    # Удаление пунктуации\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "\n",
    "    # Токенизация\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # Удаление стоп-слов и лемматизация\n",
    "    tokens = [\n",
    "        lemmatizer.lemmatize(token) for token in tokens if token not in stop_words\n",
    "    ]\n",
    "\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "# Пример использования\n",
    "sample_text = \"NLTK is a leading platform for building Python programs to work with human language data.\"\n",
    "processed_text = preprocess_text(sample_text)\n",
    "print(processed_text)\n",
    "\n",
    "# Векторизация с использованием TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "corpus = [\n",
    "    processed_text\n",
    "]  # Это пример; в реальной ситуации здесь может быть список обработанных документов\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание рекомендательной системы на основе LDA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Тренировка LDA модели\n",
    "\n",
    "Предполагается, что данные уже предобработаны с использованием вышеуказанного кода, и у нас есть список токенизированных документов texts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тренировка LDA модели\n",
    "from gensim import corpora, models\n",
    "from gensim.similarities import MatrixSimilarity\n",
    "\n",
    "# Создаем словарь и корпус\n",
    "X = [\n",
    "    \"Actions speak louder than words.\",\n",
    "    \"A picture is worth a thousand words.\",\n",
    "    \"Better late than never.\",\n",
    "    \"Birds of a feather flock together.\",\n",
    "    \"Cleanliness is next to godliness.\",\n",
    "    \"Don't bite the hand that feeds you.\",\n",
    "    \"Don't count your chickens before they hatch.\",\n",
    "    \"Don't put all your eggs in one basket.\",\n",
    "    \"Every cloud has a silver lining.\",\n",
    "    \"Honesty is the best policy.\",\n",
    "    \"If it ain't broke, don't fix it.\",\n",
    "    \"It's raining cats and dogs.\",\n",
    "    \"Look before you leap.\",\n",
    "    \"Practice makes perfect.\",\n",
    "    \"The early bird catches the worm.\",\n",
    "    \"The pen is mightier than the sword.\",\n",
    "    \"Two heads are better than one.\",\n",
    "    \"Where there's smoke, there's fire.\",\n",
    "    \"You can't judge a book by its cover.\",\n",
    "    \"When in Rome, do as the Romans do.\",\n",
    "]\n",
    "# Применяем функцию предобработки к каждому тексту и получаем список токенов для каждого текста\n",
    "processed_X = [preprocess_text(text).split() for text in X]\n",
    "\n",
    "# Создаем словарь и корпус\n",
    "dictionary = corpora.Dictionary(processed_X)\n",
    "corpus = [dictionary.doc2bow(text) for text in processed_X]\n",
    "\n",
    "\n",
    "# Тренируем LDA модель\n",
    "lda_model = models.LdaModel(corpus, num_topics=15, id2word=dictionary, passes=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Рекомендация документов для запроса\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 5\n",
    "\n",
    "\n",
    "def recommend_docs(query, lda_model, corpus, texts, top_n=5):\n",
    "    query_bow = dictionary.doc2bow(query)\n",
    "    query_lda = lda_model[query_bow]\n",
    "\n",
    "    # Вычисляем схожесть между запросом и документами\n",
    "    index = MatrixSimilarity(lda_model[corpus])\n",
    "    sims = index[query_lda]\n",
    "\n",
    "    # Сортируем документы по убыванию схожести\n",
    "    sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "    # Используем фильтрацию\n",
    "    # threshold = 0.7\n",
    "    # sims = [s for s in sims if s[1] > threshold]\n",
    "    recommended_docs = [texts[i[0]] for i in sims[:top_n]]\n",
    "    return recommended_docs\n",
    "\n",
    "\n",
    "texts = X\n",
    "\n",
    "# Пример запроса\n",
    "query = [\"best\", \"policy\"]\n",
    "recommended_texts = recommend_docs(query, lda_model, corpus, texts, top_n)\n",
    "\n",
    "for text in recommended_texts:\n",
    "    print(\"Recommended text:\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обработка русского текста\n",
    "\n",
    "1. Для русских стоп-слов вы можете использовать:\n",
    "\n",
    "```python\n",
    "stop_words = set(stopwords.words(\"russian\"))\n",
    "```\n",
    "\n",
    "2. Лемматизация русского текста сложнее. NLTK не предоставляет хороших инструментов для лемматизации русского текста. Вместо этого вы можете использовать библиотеки, такие как pymorphy2 или mystem3.\n",
    "\n",
    "3. Обратите внимание, что если вы используете LDA модель с русским текстом, вам также понадобится русскоязычный словарь и корпус.\n",
    "\n",
    "4. Пример корпуса:\n",
    "\n",
    "```python\n",
    "X = [\"Я помню чудное мгновенье:\",\n",
    "\"Передо мной явилась ты,\",\n",
    "\"Как мимолетное виденье,\",\n",
    "\"Как гений чистой красоты.\",\n",
    "\"В томленьях грусти безнадежной,\",\n",
    "\"В тревогах шумной суеты,\",\n",
    "\"Звучал мне долго голос нежный\",\n",
    "\"И снились милые черты.\",\n",
    "\"Шли годы. Бурь порыв мятежный\",\n",
    "\"Рассеял прежние мечты,\",\n",
    "\"И я забыл твой голос нежный,\",\n",
    "\"Твои небесные черты.\",\n",
    "\"В глуши, во мраке заточенья\",\n",
    "\"Тянулись тихо дни мои\",\n",
    "\"Без божества, без вдохновенья,\",\n",
    "\"Без слез, без жизни, без любви.\"]\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "337da0f49f5ad7066e3c06ffc70d3eac9274cd9d1c7f31afd8406ce9060917c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
